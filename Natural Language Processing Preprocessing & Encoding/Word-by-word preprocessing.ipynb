{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 단어 토큰화(Word Tokenization)\n",
        "## 자연어 전처리 과정들\n",
        "자연어 전처리에는 다음과 같은 단계들이 있습니다.\n",
        "\n",
        "- 토큰화: 자연어 데이터를 분석을 위한 작은 단위(토큰)로 분리합니다.\n",
        "- 정제: 분석에 큰 의미가 없는 데이터들을 제거합니다.\n",
        "- 정규화: 표현 방법이 다르지만 의미가 같은 단어들을 통합시킵니다.\n",
        "- 정수 인코딩: 컴퓨터가 이해하기 쉽도록 자연어 데이터에 정수 인덱스를 부여합니다.\n",
        "\n",
        "분석에 활용하기 위한 자연어 데이터를 코퍼스(Corpus)라고 합니다. 한국어로는 말뭉치.\n",
        "\n",
        "Corpus를 분석하기 위하여 작은 단위로 나누너어야 하는데, 이러한 단위를 토큰(Token)이라고 하고, 하나의 코퍼스를 여러 개의 토큰으로 나누는 과정을 토큰화(Tokenization)라고 합니다.\n",
        "## NLTK 설치\n",
        "영어 자연어 처리에는 NLTK라는 패키지가 많이 사용됩니다. 데이터 사이언스 시작하기 토픽에서 아나콘다를 설치했다면 NLTK는 자동으로 설치되어 있을텐데요. 혹시, 수동으로 설치가 필요하다면 주피터 노트북 셀에 아래 커맨드를 실행해 주세요."
      ],
      "metadata": {
        "id": "rgEVMAq2xz_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5DBxfUxyhqR",
        "outputId": "97be8a02-0859-4774-8495-e4067b6751b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk 패키지와 단어 토큰화에 사용될 word_tokenize() 함수\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "x6uLtTuZyoxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에서 제공하는 토큰화 모듈인 punkt를 다운로드 = 마침표 & 언어적 특성을 고려하여 토큰화를 할 수 있게 해주는 모듈\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBNsSnHyytJw",
        "outputId": "792d71a3-0325-430b-950d-5760acd08330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 토큰화 하기\n",
        "- word_tokenize()는 코퍼스를 파라미터로 넘겨서 토큰화 된 단어 리스트를 반환하는 함수"
      ],
      "metadata": {
        "id": "p_TRPHzezAPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n",
        "\n",
        "# 단어 토큰화\n",
        "tokenized_words = word_tokenize(text)\n",
        "\n",
        "print(tokenized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxD7QIqYzFCI",
        "outputId": "c502cfec-f189-479e-b02c-bd76cd2d3879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 단어 토큰화 실습\n",
        "text.py 파일에는 영어 자연어 코퍼스가 있습니다. 이상한 나라의 앨리스 본문 중 일부인데요. 해당 코퍼스를 전처리해 보겠습니다."
      ],
      "metadata": {
        "id": "Pi-qel1i3iRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EabctJGf3m8-",
        "outputId": "b4e7d8e4-6f2d-417c-df03-8de0781678b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Codeit에서는 from text import TEXT 모듈을 사용했지만. Colab 환경에서 부르는 방법을 몰라서 파일 내용 그래도 복사해옴\n",
        "corpus = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ddPBhiJ230Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "tokenized_words = word_tokenize(corpus)\n",
        "\n",
        "# 테스트 코드\n",
        "tokenized_words"
      ],
      "metadata": {
        "id": "f8KscJ_Q36e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "코퍼스에는 아무 의미도 없거나 분석의 목적에 적합하지 않은 단어들도 포함된다. 이런 단어들은 전처리 과정에서 제거해야 하는데요. 그 과정을 정제(Cleaning)라고 한다.\n",
        "\n",
        "자연어 데이터를 정제하는 방법은 여러가지인데요. 그 중에서도 등장 빈도, 단어 길이, 불용어 등을 기준으로 많이 사용."
      ],
      "metadata": {
        "id": "w-Oc4EYS1Dxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 정제 (Cleaning)\n",
        "자연어 데이터를 정제하는 방법은 여러가지. 그 중에서도 등장 빈도, 단어 길이, 불용어 등을 기준으로 많이 사용\n",
        "\n",
        "## 등장 빈도가 적은 단어\n",
        "코퍼스에 등장하는 빈도가 너무 적은 단어는 분석에 도움이 되지 않기 때문에 제거해야 한다. 데이터는 실습 레슨에서 사용했던 text.py 데이터를 그대로 사용\n"
      ],
      "metadata": {
        "id": "WIl08ZHe1MZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab에서 파일을 불러오는 경\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4G1Qo9J11TX",
        "outputId": "49cba6b5-9782-475e-9ad2-c4352c72c446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab_Notebooks/text.py'\n",
        "with open(file_path, 'r', encoding=\"utf8\") as file:\n",
        "  lines = file.readline()\n",
        "\n",
        "for line in lines:\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "MVw_vhZU1WTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codeit에서는 from text import TEXT 모듈을 사용했지만. Colab 환경에서 부르는 방법을 몰라서 파일 내용 그래도 복사해옴\n",
        "corpus = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vtcH3oCf5jgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7PUXAwT3RS3",
        "outputId": "36c00b52-ff51-479c-aec7-d194f4f5600d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "굉장히 긴 본문인데 해당 본문에서 등장 빈도가 2 이하인 단어들만 찾아봄."
      ],
      "metadata": {
        "id": "1PvkmOCP5mX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counter모듈 import\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4OUzuwes5sJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈도수 계산을 위해선 먼저 코퍼스를 단어 기준으로 토큰화 해야 하는데요. 앞에서 배운 word_tokenize() 함수로 단어 토큰화"
      ],
      "metadata": {
        "id": "HBe-kjTZ6d-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 단어 토큰 리스트\n",
        "tokenized_words = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "5SD3QCH16ZhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리고, 어의 등장 빈도를 Counter() 함수로 계산합니다. Counter()는 파라미터로 단어 리스트를 받고, 각 단어의 등장 빈도를 딕셔너리({단어: 등장 횟수}) 형태로 반환"
      ],
      "metadata": {
        "id": "SJLJPFeV7C51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
        "vocab = Counter(tokenized_words)"
      ],
      "metadata": {
        "id": "__B2Imm67FMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab은 다음과 같은 형태로 저장되어 있습니다.\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qwSulxw7G5l",
        "outputId": "91c28606-fce1-48f4-b189-f1ce67004dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'the': 30, '.': 28, ',': 21, 'of': 15, 'and': 14, 'to': 13, 'a': 12, 'military': 12, 'in': 12, 'people': 9, 'on': 9, 'are': 9, 'for': 7, 'this': 7, 'that': 6, 'I': 5, 'The': 5, 'you': 5, 'not': 4, 'or': 4, 'about': 4, 'US': 4, 'at': 4, 'every': 4, 'it': 4, 'make': 4, 'was': 4, 'movie': 3, 'be': 3, 'who': 3, 'they': 3, 'Abu-Gharib': 3, 'makes': 3, 'number': 3, 'million': 3, 'with': 3, 'total': 3, 'would': 3, 'an': 3, 'there': 3, 'days': 3, 'hour': 3, 'minimum': 3, 'get': 3, 'comments': 2, ')': 2, 'know': 2, 'nothing': 2, 'base': 2, 'state': 2, 'world': 2, 'time': 2, ':': 2, '2.3': 2, 'indicted': 2, 'than': 2, 'That': 2, \"'s\": 2, 'but': 2, 'reality': 2, 'is': 2, 'first': 2, 'aid': 2, 'When': 2, 'their': 2, 'Within': 2, 'hours': 2, 'food': 2, 'months': 2, 'But': 2, 'website': 2, 'men': 2, 'women': 2, 'so': 2, 'personal': 2, 'gain': 2, 'under': 2, '40': 2, 'work': 2, 'week': 2, 'much': 2, 'ranks': 2, 'degrees': 2, 'After': 1, 'reading': 1, 'am': 1, 'sure': 1, 'whether': 1, 'should': 1, 'angry': 1, 'sad': 1, 'sickened': 1, 'Seeing': 1, 'typical': 1, 'absolutely': 1, 'b': 1, 'everything': 1, 'think': 1, 'movies': 1, 'like': 1, 'CNN': 1, 'reports': 1, 'me': 1, 'wonder': 1, 'intellectual': 1, 'stimulation': 1, 'At': 1, 'type': 1, '1.4': 1, 'Active': 1, 'Duty': 1, 'another': 1, 'almost': 1, '900,000': 1, 'Guard': 1, 'Reserves': 1, 'roughly': 1, 'abuses': 1, 'Currently': 1, 'less': 1, '20': 1, '.00083': 1, '%': 1, 'Even': 1, 'if': 1, 'indict': 1, 'single': 1, 'member': 1, 'ever': 1, 'stepped': 1, 'come': 1, 'close': 1, 'making': 1, 'whole': 1, 'flaws': 1, 'take': 1, 'YEARS': 1, 'cover': 1, 'understand': 1, 'supposed': 1, 'sarcastic': 1, 'writer': 1, 'director': 1, 'trying': 1, 'commentary': 1, 'without': 1, 'enemy': 1, 'fight': 1, 'In': 1, 'has': 1, 'been': 1, 'its': 1, 'busiest': 1, 'when': 1, 'conflicts': 1, 'going': 1, 'called': 1, 'disaster': 1, 'relief': 1, 'humanitarian': 1, 'missions': 1, 'tsunami': 1, 'hit': 1, 'Indonesia': 1, 'devestating': 1, 'region': 1, 'scene': 1, 'chaos': 1, 'situation': 1, 'overwhelmed': 1, 'local': 1, 'governments': 1, 'leadership': 1, 'looked': 1, 'same': 1, 'mocks': 1, 'said': 1, 'happen': 1, 'reaching': 1, 'isolated': 1, 'villages': 1, 'airfields': 1, 'were': 1, 'built': 1, 'cargo': 1, 'aircraft': 1, 'started': 1, 'landing': 1, 'distribution': 1, 'system': 1, 'up': 1, 'running': 1, 'Hours': 1, 'weeks': 1, 'Yes': 1, 'unscrupulous': 1, 'then': 1, 'walk': 1, 'life': 1, 'occupation': 1, 'see': 1, 'decide': 1, 'all': 1, 'criminal': 1, 'minds': 1, 'thoughts': 1, 'destruction': 1, 'mayhem': 1, 'absolute': 1, 'disservice': 1, 'things': 1, 'do': 1, 'day': 1, 'One': 1, 'person': 1, 'even': 1, 'went': 1, 'far': 1, 'as': 1, 'say': 1, 'members': 1, 'Wow': 1, '!': 1, 'Entry': 1, 'level': 1, 'personnel': 1, 'just': 1, '$': 1, '8.00': 1, 'assuming': 1, 'Of': 1, 'course': 1, 'many': 1, 'more': 1, 'those': 1, 'harm': 1, 'way': 1, 'typically': 1, 'put': 1, '16-18': 1, 'end': 1, 'pay': 1, 'well': 1, 'wage': 1, 'So': 1, 'beg': 1, 'please': 1, 'yourself': 1, 'familiar': 1, 'around': 1, 'Go': 1, 'nearby': 1, 'visitor': 1, 'pass': 1, 'meet': 1, 'some': 1, 'quick': 1, 'disparage': 1, 'You': 1, 'surprised': 1, 'no': 1, 'longer': 1, 'accepts': 1, 'lieu': 1, 'prison': 1, 'They': 1, 'require': 1, 'GED': 1, 'prefer': 1, 'high': 1, 'school': 1, 'diploma': 1, 'middle': 1, 'expected': 1, 'undergraduate': 1, 'upper': 1, 'encouraged': 1, 'advanced': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "생성된 단어 집합에서 빈도수가 2 이하인 단어 리스트만 추출. 해당 작업에는 list comprehension 문법을 사용"
      ],
      "metadata": {
        "id": "OqQUnWAN7Mf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 2 이하인 단어 리스트 추출, uncommon_words -> 2개 이하로 등장한 단어들의 모음집\n",
        "uncommon_words = [key for key, value in vocab.items() if value <= 2]"
      ],
      "metadata": {
        "id": "L3Z3OnLt7Qal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 등장 빈도가 2 이하인 단어 리스트가 몇 개인지 확인\n",
        "print('빈도수가 2 이하인 단어 수:', len(uncommon_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNsT-LJW7TLt",
        "outputId": "0621b86c-5b7c-48b1-e7e8-31a22eca6c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "빈도수가 2 이하인 단어 수: 234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈도수가 2 이하인 단어 수: 234개 포함. 그러면 uncommon_words에 없는 단어들만 cleaned_words에 저장하면 3회 이상 등장한 단어들만 남게 되겠죠?\n",
        "\n",
        "위의 예시에서는 정제를 위한 기준을 등장 빈도 2회 이하로 설정, 그렇지만 코퍼스의 특징과 분석의 목적에 따라 몇 회 이하 등장하는 단어를 정제할지는 달라진다. 그래서 정제의 기준이 되는 숫자는 가장 적절한 숫자를 임의로 설정하면 됨."
      ],
      "metadata": {
        "id": "OOXZ2t5o7hJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 등장 빈도가 3회 이상인 단어 총 306개가 cleaned_by_freq에 저장.\n",
        "cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]\n",
        "\n",
        "print('빈도수 3 이상인 토큰 수:', len(cleaned_by_freq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW5V-40i8EyN",
        "outputId": "0c05c7c5-ad6d-4254-abfd-57bd33fc9ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "빈도수 3 이상인 토큰 수: 306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 길이가 짧은 단어\n",
        "영어 단어의 경우, 알파벳 하나 또는 두개로 구성된 단어는 코퍼스의 의미를 나타내는데 중요하지 않을 가능성이 높습니다. 그래서 이런 단어들은 제거하는 게 좋습니다.\n",
        "\n",
        "아래는 길이가 2 이하인 단어들을 제거하는 코드입니다. 단어 토큰들을 순회하면서 단어의 길이(len(word))가 2보다 큰 단어들만 cleaned_words에 넣으면 됩니다."
      ],
      "metadata": {
        "id": "Pno7caI98-0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 등장빈도가 3회 이상인 단어중 길이가 2 이하인 단어 제거\n",
        "cleaned_by_freq_len = []\n",
        "\n",
        "for word in cleaned_by_freq:\n",
        "    if len(word) > 2:\n",
        "        cleaned_by_freq_len.append(word)"
      ],
      "metadata": {
        "id": "Pjjqp8a99D1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('정제 전:', cleaned_by_freq[:10])\n",
        "print('정제 후:', cleaned_by_freq_len[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4PhzKJf9MRs",
        "outputId": "2cc6ac95-24ca-4249-b5df-2a0597f82a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정제 전: ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',']\n",
            "정제 후: ['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 정제 함수"
      ],
      "metadata": {
        "id": "xhCbtbWH9VQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 등장 빈도 기준 정제 함수\n",
        "def clean_by_freq(tokenized_words, cut_off_count):\n",
        "    # 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
        "    vocab = Counter(tokenized_words)\n",
        "\n",
        "    # 빈도수가 cut_off_count 이하인 단어 set 추출\n",
        "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
        "\n",
        "    # uncommon_words에 포함되지 않는 단어 리스트 생성\n",
        "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "# 단어 길이 기준 정제 함수\n",
        "def clean_by_len(tokenized_words, cut_off_length):\n",
        "    # 길이가 cut_off_length 이하인 단어 제거\n",
        "    cleaned_by_freq_len = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        if len(word) > cut_off_length:\n",
        "            cleaned_by_freq_len.append(word)\n",
        "\n",
        "    return cleaned_by_freq_len"
      ],
      "metadata": {
        "id": "Lk7gqQeN9YSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 정제 실습\n",
        "등장 빈도와 단어의 길이를 입력받고, 입력받은 수 이하인 토큰을 정제하는 함수 clean_by_freq()와 clean_by_len()을 만들어 주세요.\n",
        "\n",
        "- clean_by_freq()는 단어 토큰화된 코퍼스(tokenized_words)와 정제할 등장 빈도 기준(cut_off_count)을 파라미터로 받습니다.\n",
        "- clean_by_len()은 단어 토큰화 된 코퍼스(tokenized_words)와 정제할 단어 길이 기준(cut_off_length)을 파라미터로 받습니다.\n",
        "- 두 함수 모두 정제 후 남은 단어 토큰 리스트를 결과로 반환합니다.\n",
        "- 실제로 만든 함수가 잘 동작하는지 확인하기 위한 실행 코드도 완성해 주세요. clean_by_freq()는 파라미터로 단어 토큰화 된 리스트와 cut_off_count 값으로 2를 넣어주시고, clean_by_len()은 파라미터로 clean_by_freq()의 결과와 cut_off_length 값 2를 추가해 주세요."
      ],
      "metadata": {
        "id": "YzqXWpTb9d6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrbffyeI9oh2",
        "outputId": "c57016c7-fcd5-4a5e-9e31-97670ad5c8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "y-fPX0Uh9_RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_by_freq(tokenized_words, cut_off_count):\n",
        "    vocab = Counter(tokenized_words)\n",
        "\n",
        "    # 빈도수가 cut_off_count 이하인 단어를 제거하는 코드를 작성해 주세요\n",
        "    uncommon_words = [word for word in vocab if len(word) <= cut_off_count]\n",
        "    cleaned_words = [word for word in vocab if word not in uncommon_words]\n",
        "\n",
        "    return cleaned_words"
      ],
      "metadata": {
        "id": "s7rhEHHd-Ay8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_by_len(tokenized_words, cut_off_length):\n",
        "    cleaned_words = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "    # 길이가 cut_off_length 이하인 단어 제거하는 코드를 작성해 주세요\n",
        "      if len(word) > cut_off_length:\n",
        "        cleaned_words.append(word)\n",
        "\n",
        "    return cleaned_words"
      ],
      "metadata": {
        "id": "9BWB1G9U-Tsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제의 조건에 맞게 함수를 호출해 주세요\n",
        "clean_by_freq = clean_by_freq(tokenized_words, 2)\n",
        "cleaned_words = clean_by_len(tokenized_words, 2)\n",
        "\n",
        "cleaned_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBB16I54-WNE",
        "outputId": "b363d4db-f53d-4552-d178-d743b3cefb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['After',\n",
              " 'reading',\n",
              " 'the',\n",
              " 'comments',\n",
              " 'for',\n",
              " 'this',\n",
              " 'movie',\n",
              " 'not',\n",
              " 'sure',\n",
              " 'whether',\n",
              " 'should',\n",
              " 'angry',\n",
              " 'sad',\n",
              " 'sickened',\n",
              " 'Seeing',\n",
              " 'comments',\n",
              " 'typical',\n",
              " 'people',\n",
              " 'who',\n",
              " 'know',\n",
              " 'absolutely',\n",
              " 'nothing',\n",
              " 'about',\n",
              " 'the',\n",
              " 'military',\n",
              " 'who',\n",
              " 'base',\n",
              " 'everything',\n",
              " 'they',\n",
              " 'think',\n",
              " 'they',\n",
              " 'know',\n",
              " 'movies',\n",
              " 'like',\n",
              " 'this',\n",
              " 'CNN',\n",
              " 'reports',\n",
              " 'about',\n",
              " 'Abu-Gharib',\n",
              " 'makes',\n",
              " 'wonder',\n",
              " 'about',\n",
              " 'the',\n",
              " 'state',\n",
              " 'intellectual',\n",
              " 'stimulation',\n",
              " 'the',\n",
              " 'world',\n",
              " 'the',\n",
              " 'time',\n",
              " 'type',\n",
              " 'this',\n",
              " 'the',\n",
              " 'number',\n",
              " 'people',\n",
              " 'the',\n",
              " 'military',\n",
              " '1.4',\n",
              " 'million',\n",
              " 'Active',\n",
              " 'Duty',\n",
              " 'with',\n",
              " 'another',\n",
              " 'almost',\n",
              " '900,000',\n",
              " 'the',\n",
              " 'Guard',\n",
              " 'and',\n",
              " 'Reserves',\n",
              " 'for',\n",
              " 'total',\n",
              " 'roughly',\n",
              " '2.3',\n",
              " 'million',\n",
              " 'The',\n",
              " 'number',\n",
              " 'people',\n",
              " 'indicted',\n",
              " 'for',\n",
              " 'abuses',\n",
              " 'Abu-Gharib',\n",
              " 'Currently',\n",
              " 'less',\n",
              " 'than',\n",
              " 'That',\n",
              " 'makes',\n",
              " 'the',\n",
              " 'total',\n",
              " 'people',\n",
              " 'indicted',\n",
              " '.00083',\n",
              " 'the',\n",
              " 'total',\n",
              " 'military',\n",
              " 'Even',\n",
              " 'you',\n",
              " 'indict',\n",
              " 'every',\n",
              " 'single',\n",
              " 'military',\n",
              " 'member',\n",
              " 'that',\n",
              " 'ever',\n",
              " 'stepped',\n",
              " 'Abu-Gharib',\n",
              " 'you',\n",
              " 'would',\n",
              " 'not',\n",
              " 'come',\n",
              " 'close',\n",
              " 'making',\n",
              " 'that',\n",
              " 'whole',\n",
              " 'number',\n",
              " 'The',\n",
              " 'flaws',\n",
              " 'this',\n",
              " 'movie',\n",
              " 'would',\n",
              " 'take',\n",
              " 'YEARS',\n",
              " 'cover',\n",
              " 'understand',\n",
              " 'that',\n",
              " 'supposed',\n",
              " 'sarcastic',\n",
              " 'but',\n",
              " 'reality',\n",
              " 'the',\n",
              " 'writer',\n",
              " 'and',\n",
              " 'director',\n",
              " 'are',\n",
              " 'trying',\n",
              " 'make',\n",
              " 'commentary',\n",
              " 'about',\n",
              " 'the',\n",
              " 'state',\n",
              " 'the',\n",
              " 'military',\n",
              " 'without',\n",
              " 'enemy',\n",
              " 'fight',\n",
              " 'reality',\n",
              " 'the',\n",
              " 'military',\n",
              " 'has',\n",
              " 'been',\n",
              " 'its',\n",
              " 'busiest',\n",
              " 'when',\n",
              " 'there',\n",
              " 'are',\n",
              " 'not',\n",
              " 'conflicts',\n",
              " 'going',\n",
              " 'The',\n",
              " 'military',\n",
              " 'the',\n",
              " 'first',\n",
              " 'called',\n",
              " 'for',\n",
              " 'disaster',\n",
              " 'relief',\n",
              " 'and',\n",
              " 'humanitarian',\n",
              " 'aid',\n",
              " 'missions',\n",
              " 'When',\n",
              " 'the',\n",
              " 'tsunami',\n",
              " 'hit',\n",
              " 'Indonesia',\n",
              " 'devestating',\n",
              " 'the',\n",
              " 'region',\n",
              " 'the',\n",
              " 'military',\n",
              " 'was',\n",
              " 'the',\n",
              " 'first',\n",
              " 'the',\n",
              " 'scene',\n",
              " 'When',\n",
              " 'the',\n",
              " 'chaos',\n",
              " 'the',\n",
              " 'situation',\n",
              " 'overwhelmed',\n",
              " 'the',\n",
              " 'local',\n",
              " 'governments',\n",
              " 'was',\n",
              " 'military',\n",
              " 'leadership',\n",
              " 'who',\n",
              " 'looked',\n",
              " 'their',\n",
              " 'people',\n",
              " 'the',\n",
              " 'same',\n",
              " 'people',\n",
              " 'this',\n",
              " 'movie',\n",
              " 'mocks',\n",
              " 'and',\n",
              " 'said',\n",
              " 'make',\n",
              " 'happen',\n",
              " 'Within',\n",
              " 'hours',\n",
              " 'food',\n",
              " 'aid',\n",
              " 'was',\n",
              " 'reaching',\n",
              " 'isolated',\n",
              " 'villages',\n",
              " 'Within',\n",
              " 'days',\n",
              " 'airfields',\n",
              " 'were',\n",
              " 'built',\n",
              " 'cargo',\n",
              " 'aircraft',\n",
              " 'started',\n",
              " 'landing',\n",
              " 'and',\n",
              " 'food',\n",
              " 'distribution',\n",
              " 'system',\n",
              " 'was',\n",
              " 'and',\n",
              " 'running',\n",
              " 'Hours',\n",
              " 'and',\n",
              " 'days',\n",
              " 'not',\n",
              " 'weeks',\n",
              " 'and',\n",
              " 'months',\n",
              " 'Yes',\n",
              " 'there',\n",
              " 'are',\n",
              " 'unscrupulous',\n",
              " 'people',\n",
              " 'the',\n",
              " 'military',\n",
              " 'But',\n",
              " 'then',\n",
              " 'there',\n",
              " 'are',\n",
              " 'every',\n",
              " 'walk',\n",
              " 'life',\n",
              " 'every',\n",
              " 'occupation',\n",
              " 'But',\n",
              " 'see',\n",
              " 'people',\n",
              " 'this',\n",
              " 'website',\n",
              " 'decide',\n",
              " 'that',\n",
              " '2.3',\n",
              " 'million',\n",
              " 'men',\n",
              " 'and',\n",
              " 'women',\n",
              " 'are',\n",
              " 'all',\n",
              " 'criminal',\n",
              " 'with',\n",
              " 'nothing',\n",
              " 'their',\n",
              " 'minds',\n",
              " 'but',\n",
              " 'thoughts',\n",
              " 'destruction',\n",
              " 'mayhem',\n",
              " 'absolute',\n",
              " 'disservice',\n",
              " 'the',\n",
              " 'things',\n",
              " 'that',\n",
              " 'they',\n",
              " 'every',\n",
              " 'day',\n",
              " 'One',\n",
              " 'person',\n",
              " 'this',\n",
              " 'website',\n",
              " 'even',\n",
              " 'went',\n",
              " 'far',\n",
              " 'say',\n",
              " 'that',\n",
              " 'military',\n",
              " 'members',\n",
              " 'are',\n",
              " 'for',\n",
              " 'personal',\n",
              " 'gain',\n",
              " 'Wow',\n",
              " 'Entry',\n",
              " 'level',\n",
              " 'personnel',\n",
              " 'make',\n",
              " 'just',\n",
              " 'under',\n",
              " '8.00',\n",
              " 'hour',\n",
              " 'assuming',\n",
              " 'hour',\n",
              " 'work',\n",
              " 'week',\n",
              " 'course',\n",
              " 'many',\n",
              " 'work',\n",
              " 'much',\n",
              " 'more',\n",
              " 'than',\n",
              " 'hours',\n",
              " 'week',\n",
              " 'and',\n",
              " 'those',\n",
              " 'harm',\n",
              " 'way',\n",
              " 'typically',\n",
              " 'put',\n",
              " '16-18',\n",
              " 'hour',\n",
              " 'days',\n",
              " 'for',\n",
              " 'months',\n",
              " 'end',\n",
              " 'That',\n",
              " 'makes',\n",
              " 'the',\n",
              " 'pay',\n",
              " 'well',\n",
              " 'under',\n",
              " 'minimum',\n",
              " 'wage',\n",
              " 'much',\n",
              " 'for',\n",
              " 'personal',\n",
              " 'gain',\n",
              " 'beg',\n",
              " 'you',\n",
              " 'please',\n",
              " 'make',\n",
              " 'yourself',\n",
              " 'familiar',\n",
              " 'with',\n",
              " 'the',\n",
              " 'world',\n",
              " 'around',\n",
              " 'you',\n",
              " 'nearby',\n",
              " 'base',\n",
              " 'get',\n",
              " 'visitor',\n",
              " 'pass',\n",
              " 'and',\n",
              " 'meet',\n",
              " 'some',\n",
              " 'the',\n",
              " 'men',\n",
              " 'and',\n",
              " 'women',\n",
              " 'you',\n",
              " 'are',\n",
              " 'quick',\n",
              " 'disparage',\n",
              " 'You',\n",
              " 'would',\n",
              " 'surprised',\n",
              " 'The',\n",
              " 'military',\n",
              " 'longer',\n",
              " 'accepts',\n",
              " 'people',\n",
              " 'lieu',\n",
              " 'prison',\n",
              " 'time',\n",
              " 'They',\n",
              " 'require',\n",
              " 'minimum',\n",
              " 'GED',\n",
              " 'and',\n",
              " 'prefer',\n",
              " 'high',\n",
              " 'school',\n",
              " 'diploma',\n",
              " 'The',\n",
              " 'middle',\n",
              " 'ranks',\n",
              " 'are',\n",
              " 'expected',\n",
              " 'get',\n",
              " 'minimum',\n",
              " 'undergraduate',\n",
              " 'degrees',\n",
              " 'and',\n",
              " 'the',\n",
              " 'upper',\n",
              " 'ranks',\n",
              " 'are',\n",
              " 'encouraged',\n",
              " 'get',\n",
              " 'advanced',\n",
              " 'degrees']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 불용어(Stopwords)\n",
        "코퍼스에서 큰 의미가 없거나, 분석 목적에서 벗어나는 단어들을 불용어(stopword)라고 한다. -> 정확한 분석을 방해하기 때문에 제거해야 한다.\n",
        "\n",
        "# 용어 제거는 다음과 같은 방식으로 진행된다.\n",
        "\n",
        "- 불용어를 모아 놓은 불용어 세트 준비\n",
        "- 코퍼스의 각 단어 토큰이 불용어 세트에 포함되는지 확인\n",
        "- 불용어 세트에 있는 단어 토큰은 분석에서 제외\n",
        "\n",
        "NLTK는 기본 불용어 목록 179개를 제공합니다. 해당 불용어 목록은 'stopwords.words('english')' 로 접근할 수 있다.\n"
      ],
      "metadata": {
        "id": "93PoE7hl_YnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUCgWTMz_0WM",
        "outputId": "b7d8ccd4-0ca1-4c3f-d1c0-0e1993975c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "print('불용어 개수 :', len(stopwords_set))\n",
        "print(stopwords_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORTM7nxi_66D",
        "outputId": "5713ea50-609a-408a-f261-aa3dbeb857f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "{'after', 're', 'where', 'its', \"shouldn't\", 'at', 'hasn', 'wasn', 'we', 'than', 'their', 'can', 'me', 'her', 'that', 'too', \"don't\", 'he', \"should've\", 'down', 've', 'weren', 'shouldn', 'under', 'should', \"you'd\", \"couldn't\", 'into', 'is', 'any', 'most', 'didn', 'doesn', \"mightn't\", 'ourselves', 'same', 'when', 'again', 'our', 'those', 'out', 'there', \"mustn't\", 'o', \"she's\", 'up', 'mightn', \"you'll\", 'which', 'himself', 'aren', 'until', 'they', 'wouldn', 'itself', 'on', 'why', \"that'll\", 'am', 'so', 'haven', 'themselves', \"wasn't\", 'ours', 'nor', 'not', \"won't\", 'whom', 'between', 'd', 'couldn', 'were', 'other', 'a', 'i', 'but', 'for', 'only', 'or', 'all', 'such', \"haven't\", 'over', 'being', 'yourself', 'it', \"wouldn't\", 'very', 'before', 'she', 'who', 'if', 'with', 'having', 'against', 'in', 'don', \"isn't\", 'him', 'because', 'as', 'of', 'theirs', 'this', 'an', 'are', 'from', 'how', 'be', 'some', 'won', \"shan't\", 'doing', 'below', 'just', 'have', 'yourselves', 'his', 'during', 'once', 's', 'further', 'by', 'about', 'ma', 'does', 'shan', 'here', \"hasn't\", 'them', 'do', 'hadn', 'herself', 'my', 'isn', 'has', 'few', 'to', 'yours', \"you're\", \"weren't\", 'myself', 'while', 'now', \"didn't\", 'more', 'ain', 'm', 'hers', 'been', 'was', 'did', 'no', 'these', \"needn't\", 'll', 'mustn', 'what', \"you've\", 'above', 'both', 'each', 'own', \"doesn't\", 'through', \"it's\", 'the', 'will', 'you', 'and', 'then', 'y', 'your', 't', 'needn', 'off', \"aren't\", 'had', \"hadn't\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "일반적인 코퍼스에서 많이 사용되지만 분석에 크게 활용되지 않는 단어들이 불용어 세트에 포함. 경우에 따라서는 NLTK에서 기본 제공하는 불용어에 새로운 단어를 추가하거나, 일부 단어를 기본 불용어 목록에서 제거해야 할 수도 있죠. 이런 경우에는 세트 데이터 형식의 add(), remove() 함수를 사용한다.\n",
        "\n",
        "한번 stopwords_set에 원하는 불용어를 추가하고 삭제해 볼게요."
      ],
      "metadata": {
        "id": "wiI8Yig9ABs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에서 받아온 불용어 세트 stopwords_set에 hello가 추가되었고, the와 me가 제거\n",
        "# 또, NLTK가 기본 제공하는 불용어가 아니라 새로운 불용어 세트를 정의해서 사용할 수도 있습니다.\n",
        "stopwords_set.add('hello')\n",
        "stopwords_set.remove('the')\n",
        "stopwords_set.remove('me')\n",
        "\n",
        "print('불용어 개수 :', len(stopwords_set))\n",
        "print('불용어 출력 :',stopwords_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5J-Gh9QAKIL",
        "outputId": "ef133ae0-1282-4e76-fab1-02866f175109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 178\n",
            "불용어 출력 : {'after', 're', 'where', 'its', \"shouldn't\", 'at', 'hasn', 'wasn', 'we', 'than', 'their', 'can', 'her', 'that', 'too', \"don't\", 'he', \"should've\", 'down', 've', 'weren', 'shouldn', 'under', 'should', \"you'd\", \"couldn't\", 'into', 'is', 'any', 'most', 'didn', 'doesn', \"mightn't\", 'ourselves', 'same', 'hello', 'when', 'again', 'our', 'those', 'out', 'there', \"mustn't\", 'o', \"she's\", 'up', 'mightn', \"you'll\", 'which', 'himself', 'aren', 'until', 'they', 'wouldn', 'itself', 'on', 'why', \"that'll\", 'am', 'so', 'haven', 'themselves', \"wasn't\", 'ours', 'nor', 'not', \"won't\", 'whom', 'between', 'd', 'couldn', 'were', 'other', 'a', 'i', 'but', 'for', 'only', 'or', 'all', 'such', \"haven't\", 'over', 'being', 'yourself', 'it', \"wouldn't\", 'very', 'before', 'she', 'who', 'if', 'with', 'having', 'against', 'in', 'don', \"isn't\", 'him', 'because', 'as', 'of', 'theirs', 'this', 'an', 'are', 'from', 'how', 'be', 'some', 'won', \"shan't\", 'doing', 'below', 'just', 'have', 'yourselves', 'his', 'during', 'once', 's', 'further', 'by', 'about', 'ma', 'does', 'shan', 'here', \"hasn't\", 'them', 'do', 'hadn', 'herself', 'my', 'isn', 'has', 'few', 'to', 'yours', \"you're\", \"weren't\", 'myself', 'while', 'now', \"didn't\", 'more', 'ain', 'm', 'hers', 'been', 'was', 'did', 'no', 'these', \"needn't\", 'll', 'mustn', 'what', \"you've\", 'above', 'both', 'each', 'own', \"doesn't\", 'through', \"it's\", 'will', 'you', 'and', 'then', 'y', 'your', 't', 'needn', 'off', \"aren't\", 'had', \"hadn't\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_stopwords_set = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves'}\n",
        "\n",
        "print(my_stopwords_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiuaW4t-A1Dz",
        "outputId": "c8119853-0ea5-4455-a593-1fca7d18bff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ours', 'me', 'my', 'myself', 'i', 'our', 'we', 'ourselves'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 불용어(Stopwords) 제거하기\n",
        "\n",
        "등장 빈도와 단어 길이 기준으로 정제했던 cleaned_by_freq_len에서 불용어도 제거해 보겠습니다. cleaned_by_freq_len에 있는 토큰을 하나씩 순회하면서 단어가 불용어 세트에 있는지 확인하고, 없을 때에만 cleaned_words에 저장해 줄게요."
      ],
      "metadata": {
        "id": "h9BAqH2DA-LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_set = set(stopwords.words('english'))\n",
        "\n",
        "# 불용어 제거\n",
        "cleaned_words = []\n",
        "\n",
        "for word in cleaned_by_freq_len:\n",
        "    if word not in stop_words_set:\n",
        "        cleaned_words.append(word)"
      ],
      "metadata": {
        "id": "ZEIQXnURBMzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 제거 결과 확인\n",
        "print('불용어 제거 전:', len(cleaned_by_freq_len))\n",
        "print('불용어 제거 후:', len(cleaned_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euTFpNTtBPom",
        "outputId": "2e991940-43a9-4599-dc34-22f3937a1dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전: 169\n",
            "불용어 제거 후: 67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 불용어 처리 함수"
      ],
      "metadata": {
        "id": "xdnQf8PxBha6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 제거 함수\n",
        "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
        "    cleaned_words = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        if word not in stop_words_set:\n",
        "            cleaned_words.append(word)\n",
        "\n",
        "    return cleaned_words"
      ],
      "metadata": {
        "id": "Rj5INgbpBj0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 불용어 제거 실습\n",
        "\n",
        "불용어 제거를 위한 함수 clean_by_stopwords()를 만들어 주세요.\n",
        "\n",
        "- clean_by_stopwords()는 파라미터로 단어 토큰화된 코퍼스(tokenized_words)와 불용어 목록(stopwords_set)을 받습니다.\n",
        "- 결과로는 불용어가 제거된 단어 토큰 리스트를 반환합니다.\n",
        "- 불용어 목록은 NLTK에서 제공하는 기본 불용어 목록 세트를 받아와 사용합니다."
      ],
      "metadata": {
        "id": "CtQn-NUmB04p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_words = word_tokenize(corpus)\n",
        "\n",
        "# NLTK에서 제공하는 불용어 목록을 세트 자료형으로 받아와 주세요\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "def clean_by_stopwords(tokenized_words, stopwords_set):\n",
        "    cleaned_words = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        # 여기에 코드를 작성하세요\n",
        "        if word not in stop_words_set:\n",
        "            cleaned_words.append(word)\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "# 테스트 코드\n",
        "clean_by_stopwords(tokenized_words, stopwords_set)"
      ],
      "metadata": {
        "id": "_JF5q4dDB7CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8, 정규화(Normalization)\n",
        "아래 단어들은 형태가 조금씩 다르지만 의미는 같습니다.\n",
        "\n",
        "US, USA, U.S., USA, America ...\n",
        "이렇게 형태가 다르지만 같은 의미를 나타내는 단어들이 많아질수록 코퍼스는 복잡해지고 분석이 어려워집니다. 그래서 의미가 같은 단어라면 형태를 하나로 통일하는 게 좋습니다. 해당 과정을 정규화(Normalization)라고 합니다.\n",
        "\n",
        "정규화에는 여러 방법이 있는데요. 이번 레슨에서는 가장 보편적으로 사용되는 두 가지만 소개해 드릴게요.\n",
        "\n"
      ],
      "metadata": {
        "id": "gt0JXNNcCzxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규화 방법: 대소문자 통합\n",
        "대부분의 프로그래밍 언어는 대소문자를 구분합니다. 그래서 코퍼스를 대문자나 소문자 중 하나로 통일하면 정규화가 됩니다.\n",
        "\n",
        "영어 문법 상 대문자는 특수한 상황에서만 사용되고, 보통은 소문자가 많이 사용됩니다. 따라서 대문자를 소문자로 바꾸는게 일반적입니다. 해당 과정에는 파이썬의 문자열 내장함수인 lower()가 사용됩니다."
      ],
      "metadata": {
        "id": "qHZLs-uADCik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "text = \"What can I do for you? Do your homework now.\"\n",
        "\n",
        "# 소문자로 변환\n",
        "print(text.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMkrB2xcGM5B",
        "outputId": "bd608be2-578d-4d07-f9f0-6b0c2b7e3124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what can i do for you? do your homework now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규화 방법: 규칙 기반 정규화\n",
        "USA, US, U.S.는 형태가 다르지만 의미는 같습니다. 표준어는 아니지만 Umm과 Ummmm도 같은 의미이기 때문에 정규화 할 수 있죠. 이런 단어들은 규칙을 정의해서 하나의 표현으로 통합할 수 있습니다.\n",
        "\n",
        "아래 코드는 US를 USA로, Ummmm을 Umm으로 통합하는 코드입니다."
      ],
      "metadata": {
        "id": "5MxhSreRHq03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 동의어 사전\n",
        "synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm' }\n",
        "text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n",
        "normalized_words = []"
      ],
      "metadata": {
        "id": "gdV8QFFuHxJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로, 문장을 단어 토큰화 한 다음 토큰들을 순회하면서 동의어 사전의 키에 해당 단어가 포함되는지 확인합니다. 만약에 포함된다면 정규화할 단어로 바꿉니다."
      ],
      "metadata": {
        "id": "bOibx9tEI3au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화\n",
        "tokenized_words = nltk.word_tokenize(text)\n",
        "\n",
        "for word in tokenized_words:\n",
        "    # 동의어 사전에 있는 단어라면, value에 해당하는 값으로 변환\n",
        "    if word in synonym_dict.keys():\n",
        "        word = synonym_dict[word]\n",
        "\n",
        "    normalized_words.append(word)"
      ],
      "metadata": {
        "id": "k0IEjGEGI4x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 확인\n",
        "print(normalized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZQWYrkVJAw0",
        "outputId": "0d8f1ac3-dadd-4814-c81a-ce914bcd7cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.', 'She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. 어간추출\n",
        "\n",
        "특정한 단어의 핵심이 되는 부분을 어간(Stem)이라고 합니다. 그리고 단어에서 어간을 찾아내는 것을 어간 추출(Stemming)이라고 합니다. 서로 다른 형태의 단어들도 어간 추출을 하면 같은 단어로 통합되기 때문에 이를 정규화 방법 중 하나로 사용합니다.\n",
        "\n",
        "아래는 어간 추출 알고리즘 중 하나인 포터 스테머 알고리즘(Porter Stemmer Algorithm)의 규칙 일부입니다. 단순히 어미만 잘라내는 방식으로 어간을 찾고 있는데요. 그렇기 때문에 사전에 없는 단어가 결과로 나오기도 합니다.\n",
        "\n",
        "- alize → al (Formalize → Formal)\n",
        "- ational → ate (Relational -> Relate)\n",
        "- ate → 제거 (Activate -> Activ)\n",
        "- ment → 제거 (Encouragement -> Encourage)\n",
        "\n",
        "예를 들면 Activate에서 ate를 제거해 찾은 어간 activ는 사전에 없는 단어입니다. ate를 제거하고 뒤에 e를 붙여줘야 완전한 단어가 되지만, 그렇게까지 섬세하게 처리해 주지는 못하고 있네요.\n",
        "\n",
        "따라서 코퍼스의 특성이나 분석하는 상황에 따라 어간 추출을 하는게 적합한지를 잘 판단해야 합니다. 그렇지 않으면 분석에 활용돼야 하는 중요한 단어가 손실될 수 있습니다."
      ],
      "metadata": {
        "id": "b7oYmx1cdFWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK로 어간 추출 하기\n",
        "\n",
        "NLTK는 어간 추출을 위한 알고리즘으로 포터 스테머(Porter Stemmer)와 랭커스터 스테머(Lancaster Stemmer)를 제공합니다. 두 알고리즘은 어간 추출을 하는 기준이 미세하게 다르기 때문에 무엇을 사용하느냐에 따라 결과가 조금씩 달라집니다. 하지만 사용 방식은 거의 동일하기 때문에, 이번 레슨에서는 포터 스테머 알고리즘을 사용하는 방법만 자세히 알아 볼게요. 아래는 어간 추출을 위한 코드입니다."
      ],
      "metadata": {
        "id": "8kgNQNJbeDUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "GAEKkBTxeq64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "text = \"You are so lovely. I am loving you now.\"\n",
        "porter_stemmed_words = []"
      ],
      "metadata": {
        "id": "SJiKNWWse7c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출을 하기 전에 단어 토큰화가 되어야 한다.\n",
        "tokenized_words = nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "IM1jDSoLfVY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로, 토큰화 된 단어를 순회하며 어간을 추출하여 결과를 porter_stemmed_words에 추가합니다. 참고로, NLTK의 porter_stemmer.stem() 함수는 단어가 포터 스테머 알고리즘의 기준에 포함되면 추출된 어간을 반환하고, 그렇지 않은 경우에는 원래의 단어를 반환해 줍니다."
      ],
      "metadata": {
        "id": "YLLaZtHcffQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 포터 스테머의 어간 추출\n",
        "for word in tokenized_words:\n",
        "    stem = porter_stemmer.stem(word)\n",
        "    porter_stemmed_words.append(stem)"
      ],
      "metadata": {
        "id": "XESCaPEcfg4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lovely와 loving이 love로 어간 추출. 서로 다른 두 개의 단어가 하나의 어간으로 정규화.\n",
        "print('어간 추출 전 :', tokenized_words)\n",
        "print('포터 스테머의 어간 추출 후:', porter_stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqMGW3K3fwbA",
        "outputId": "15e3aba0-ed9a-4715-f7ef-eec52f999e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
            "포터 스테머의 어간 추출 후: ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "포터 스테머 알고리즘을 이용한 어간 추출 방법을 배웠는데요. 랭커스터 스테머 알고리즘은 동일한 코드에 적용하는 함수만 바꾸면 됩니다."
      ],
      "metadata": {
        "id": "O4x2OvjfglJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "text = \"You are so lovely. I am loving you now.\"\n",
        "lancaster_stemmed_words = []\n",
        "\n",
        "# 랭커스터 스테머의 어간 추출\n",
        "for word in tokenized_words:\n",
        "    stem = lancaster_stemmer.stem(word)\n",
        "    lancaster_stemmed_words.append(stem)"
      ],
      "metadata": {
        "id": "eCdRWGmignjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('어간 추출 전 :', tokenized_words)\n",
        "print('랭커스터 스테머의 어간 추출 후:', lancaster_stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUS-IL5sgyyg",
        "outputId": "7bdb7d0d-3498-4116-e8bd-70fc34e08bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
            "랭커스터 스테머의 어간 추출 후: ['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 하는 코드 -> 함수화\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# 포터 스테머 어간 추출 함수\n",
        "def stemming_by_porter(tokenized_words):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    porter_stemmed_words = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        stem = porter_stemmer.stem(word)\n",
        "        porter_stemmed_words.append(stem)\n",
        "\n",
        "    return porter_stemmed_words"
      ],
      "metadata": {
        "id": "C9kN7i5DhBd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. 어간 추출 실습\n",
        "포터 스테머 알고리즘으로 어간을 추출하는 함수 stemming_by_porter()를 만들어주세요.\n",
        "\n",
        "stemming_by_porter() 함수는 파라미터로 토큰화한 코퍼스(tokenized_words)가 전달됩니다.\n",
        "결과로는 어간이 추출된 토큰 리스트가 반환됩니다."
      ],
      "metadata": {
        "id": "Y0ZSQiHPhJ45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지와 함수 불러오기\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_words = word_tokenize(corpus)\n",
        "\n",
        "# 포터 스테머의 어간 추출\n",
        "def stemming_by_porter(tokenized_words):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    porter_stemmed_words = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        # 여기에 코드를 작성하세요\n",
        "        stem = porter_stemmer.stem(word)\n",
        "        porter_stemmed_words.append(stem)\n",
        "\n",
        "    return porter_stemmed_words\n",
        "\n",
        "stemming_by_porter(tokenized_words)"
      ],
      "metadata": {
        "id": "rBV_22HahPZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. 자연어 전처리 적용\n",
        "실습에는 IMDb 영화 리뷰 데이터를 사용하겠습니다. IMDb는 The Internet Movie Database의 약자로, 약 200만개 이상의 영화 관련 정보들이 저장되어 있는 데이터 베이스입니다.\n",
        "\n",
        "실습에는 IMDb에 있는 데이터 중 10개만 가져와서 사용하겠습니다. 아래 imdb.tsv 파일을 확인해 주세요.\n",
        "\n",
        "실무에서는 자연어 데이터를 보통 Pandas 데이터 프레임 형태로 처리합니다. 본 레슨에서도 실무 상황과 동일하게 Pandas 데이터 프레임의 각 로우에 코퍼스들을 저장하고 실습을 진행하겠습니다."
      ],
      "metadata": {
        "id": "yJBJbf-Yh-Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jX6L3-2micCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/imdb.tsv', delimiter='\\t', quoting=3)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "RAYMPNWRie0x",
        "outputId": "81ef84b4-c6dc-4979-bfd3-a6ec8cba7b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                             review\n",
              "0           0  \"Watching Time Chasers, it obvious that it was...\n",
              "1           1  I saw this film about 20 years ago and remembe...\n",
              "2           2  Minor Spoilers In New York, Joan Barnard (Elvi...\n",
              "3           3  I went to see this film with a great deal of e...\n",
              "4           4  \"Yes, I agree with everyone on this site this ...\n",
              "5           5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n",
              "6           6  Amy Poehler is a terrific comedian on Saturday...\n",
              "7           7  \"A plane carrying employees of a large biotech...\n",
              "8           8  A well made, gritty science fiction movie, it ...\n",
              "9           9  \"Incredibly dumb and utterly predictable story..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4f917fe-6a75-47a8-8b19-f804a70ff408\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I saw this film about 20 years ago and remembe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I went to see this film with a great deal of e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>\"Jennifer Ehle was sparkling in \\\"\"Pride and P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>\"A plane carrying employees of a large biotech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>A well made, gritty science fiction movie, it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>\"Incredibly dumb and utterly predictable story...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4f917fe-6a75-47a8-8b19-f804a70ff408')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f4f917fe-6a75-47a8-8b19-f804a70ff408 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f4f917fe-6a75-47a8-8b19-f804a70ff408');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 대소문자 통합\n",
        "가장 먼저 정규화를 위해 코퍼스의 대소문자를 통합해 주겠습니다. 앞선 정규화 레슨에서 대소문자 통합은 보통 대문자를 소문자로 바꾼다고 했었죠? 해당 과정을 진행해 보겠습니다.\n",
        "\n",
        "df[’review’]는 Pandas 시리즈 형식의 데이터인데요. 해당 데이터 형식으로 저장되어 있는 문자열들을 소문자로 변환하려면 str.lower() 를 사용하면 됩니다."
      ],
      "metadata": {
        "id": "s1960PZMkfde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].str.lower()\n",
        "print(df['review'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLMHi0NukiAX",
        "outputId": "e5b12c52-b3c8-4608-db61-692b0979bd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"watching time chasers, it obvious that it was made by a bunch of friends. maybe they were sitting around one day in film school and said, \\\"\"hey, let's pool our money together and make a really bad movie!\\\"\" or something like that. what ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. all corners were cut, except the one that would have prevented this film's release. life's like that.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단어 토큰화\n",
        "\n",
        "다음으로 전체 코퍼스를 단어로 토큰화해 보겠습니다. df['review']에 있는 모든 로우에 word_tokenized() 함수를 적용하면 되는데요. apply() 함수를 사용하면 그 작업을 쉽게 처리할 수 있습니다. apply()는 파라미터로 함수 이름을 전달하여 데이터 프레임 전체에 동일한 함수를 적용시켜 줍니다. 사용 방법은 아래와 같습니다."
      ],
      "metadata": {
        "id": "HTCl1ddzlCYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_tokens'] = df['review'].apply(word_tokenize)\n",
        "print(df['word_tokens'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAKUf3x8lFWn",
        "outputId": "7f164241-046e-44ac-f246-f7c0f3722831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['``', 'watching', 'time', 'chasers', ',', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends', '.', 'maybe', 'they', 'were', 'sitting', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'said', ',', '\\\\', \"''\", \"''\", 'hey', ',', 'let', \"'s\", 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', '!', '\\\\', \"''\", \"''\", 'or', 'something', 'like', 'that', '.', 'what', 'ever', 'they', 'said', ',', 'they', 'still', 'ended', 'up', 'making', 'a', 'really', 'bad', 'movie', '--', 'dull', 'story', ',', 'bad', 'script', ',', 'lame', 'acting', ',', 'poor', 'cinematography', ',', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', ',', 'etc', '.', 'all', 'corners', 'were', 'cut', ',', 'except', 'the', 'one', 'that', 'would', 'have', 'prevented', 'this', 'film', \"'s\", 'release', '.', 'life', \"'s\", 'like', 'that', '.', \"''\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 정제\n",
        "\n",
        "데이터 정제 방법으로는 등장 빈도, 단어 길이, 불용어 세트를 사용하는 방법을 배웠었죠? 해당 내용들을 모두 활용해 볼게요. 각 코퍼스별로 등장 빈도가 1회 이하, 단어의 길이가 2 이하, 그리고 NLTK에서 기본 제공하는 불용어에 해당하는 단어들을 정제해 보겠습니다.\n",
        "\n",
        "해당 과정은 preprocess.py 파일에 만들어 둔 함수 clean_by_freq(), clean_by_len(), clean_by_stopwords()를 사용하면 쉽게 처리할 수 있는데요. 불러오기 전에 꼭 아래 코드를 먼저 실행해야 합니다."
      ],
      "metadata": {
        "id": "Tc05BA30lMIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "o0jCJGwglQLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ipynb 파일에서 직접 만든 파이썬 모듈(.py)을 불러와 사용할 때, 파이썬 모듈 파일이 중간에 수정되면 해당 내용이 자동으로 반영되지 않는 문제가 있습니다. 그래서, preprocess.py 파일을 수정할 때마다 주피터 노트북의 커널을 Restart해야 하는 번거로움이 있는데요. 그런 번거로움을 줄이기 위해 위의 코드를 먼저 실행해야 합니다. 관련된 자세한 사항은 튜토리얼 레슨을 참고해 주세요.\n",
        "\n",
        "그러면 불러온 함수들을 df['word_tokens']에 apply()로 적용해 볼게요."
      ],
      "metadata": {
        "id": "5JenXXCElnSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess.py 파일\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "\n",
        "# 등장 빈도 기준 정제 함수\n",
        "def clean_by_freq(tokenized_words, cut_off_count):\n",
        "    # 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
        "    vocab = Counter(tokenized_words)\n",
        "\n",
        "    # 빈도수가 cut_off_count 이하인 단어 set 추출\n",
        "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
        "\n",
        "    # uncommon_words에 포함되지 않는 단어 리스트 생성\n",
        "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "# 단어 길이 기준 정제 함수\n",
        "def clean_by_len(tokenized_words, cut_off_length):\n",
        "    # 길이가 cut_off_length 이하인 단어 제거\n",
        "    cleaned_by_freq_len = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        if len(word) > cut_off_length:\n",
        "            cleaned_by_freq_len.append(word)\n",
        "\n",
        "    return cleaned_by_freq_len\n",
        "\n",
        "# 불용어 제거 함수\n",
        "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
        "    cleaned_words = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        if word not in stop_words_set:\n",
        "            cleaned_words.append(word)\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "# 포터 스테머 어간 추출 함수\n",
        "def stemming_by_porter(tokenized_words):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    porter_stemmed_words = []\n",
        "\n",
        "    for word in tokenized_words:\n",
        "        stem = porter_stemmer.stem(word)\n",
        "        porter_stemmed_words.append(stem)\n",
        "\n",
        "    return porter_stemmed_words"
      ],
      "metadata": {
        "id": "Y_YAQK_Qm6-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "src = list(files.upload().values())[0]\n",
        "open('preprocess.py','wb').write(src)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "sC64v2g_oCy1",
        "outputId": "ad6fb4e1-80c6-4cd8-d6a9-adbedf41c6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bc9f5696-665d-49aa-a981-dfcfc50dfeb2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bc9f5696-665d-49aa-a981-dfcfc50dfeb2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving preprocess.py to preprocess.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1562"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# .py 모듈 수정 시 자동 리로드\n",
        "from preprocess import clean_by_freq\n",
        "from preprocess import clean_by_len\n",
        "from preprocess import clean_by_stopwords"
      ],
      "metadata": {
        "id": "H97_vzVolpOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n",
        "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n",
        "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))"
      ],
      "metadata": {
        "id": "W_TJmQTilwV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만들어 둔 함수들을 데이터 프레임에 적용할 때 처음 보는 표현 방식이 사용됐는데요. lambda 파라미터: 표현식 형태로 사용된 이 부분을 람다 함수라고 부릅니다.\n",
        "\n",
        "예를 들어 아래와 같이 파라미터로 받은 두 숫자를 더하는 함수가 있다고 가정해 볼게요."
      ],
      "metadata": {
        "id": "IdqENrVoogMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plus(a, b):\n",
        "    return a+b"
      ],
      "metadata": {
        "id": "ebZp5-mooiwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "해당 함수는 람다 함수로 아래와 같이 표현할 수 있습니다."
      ],
      "metadata": {
        "id": "FyBBSrUioluN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda x, y: x + y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb5KalVMomqs",
        "outputId": "2a9be85c-53c0-4c9a-832d-982af02d877a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.<lambda>(x, y)>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 사용된 예시로도 생각해 볼게요. 아래의 람다 함수는 clean_by_freq() 함수를 실행해 주는 하나의 함수입니다. 매개변수 x에 데이터프레임의 각 행에 있는 데이터가 들어와서 clean_by_freq(x, 1)을 실행한 결과를 리턴하는 함수인거죠."
      ],
      "metadata": {
        "id": "rXgtYbzJos6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda x: clean_by_freq(x ,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_w_0rzloud1",
        "outputId": "947dcf1f-5f5e-406c-b914-762f49b2503e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.<lambda>(x)>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply()는 파라미터로 적용할 함수 이름 하나만 넣을 수 있기 때문에 clean_by_freq()처럼 두 개 이상의 파라미터가 필요한 함수를 써야 할 때에는 해당 함수를 실행하는 또 다른 함수를 람다식 형태로 만들어서 사용할 수 있습니다.\n",
        "\n",
        "적용한 결과도 한번 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "WhxqjvjAovrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['cleaned_tokens'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nJvgOquoxCd",
        "outputId": "cbca6980-3ae0-417a-ae0a-a6d0243719d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', 'film', 'said', 'really', 'bad', 'movie', 'like', 'said', 'really', 'bad', 'movie', 'bad', 'one', 'film', 'like']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##어간 추출\n",
        "마지막으로 어간 추출을 이용해 정규화해 보겠습니다. 어간 추출 레슨에서 만들었던 stemming_by_porter() 함수를 df['cleaned_tokens']에 적용해 볼게요."
      ],
      "metadata": {
        "id": "4kSUQ3qso0A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocess import stemming_by_porter\n",
        "df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)\n",
        "print(df['stemmed_tokens'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EhISTRpo4sN",
        "outputId": "0c935271-8028-4427-96d0-b098dcfac1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', 'film', 'said', 'realli', 'bad', 'movi', 'like', 'said', 'realli', 'bad', 'movi', 'bad', 'one', 'film', 'like']\n"
          ]
        }
      ]
    }
  ]
}